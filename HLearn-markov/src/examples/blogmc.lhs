
>{-# LANGUAGE DataKinds,ConstraintKinds, GADTs #-}
>
>import Control.DeepSeq
>import Control.Monad.Random
>import Data.Number.LogFloat
>import qualified Data.ByteString.Char8 as B
>import qualified Data.StorableVector as V
>
>import HLearn.Algebra
>import HLearn.Models.Markov.MarkovChain
>import qualified Control.ConstraintKinds as CK
>import Control.Parallel.Strategies
>import System.IO.Unsafe
>import Control.Concurrent

One simple use for Markov chains is analyzing sequences of DNA.
DNA consists of a sequence of four base pairs, represented by the letters A, G, C, and T.
These letters are the data points; and a data set is a string.
A very, very short strand of DNA might look something like:

>dna_short = "AGCTGCATAGCGCGATTACGATACG"

We can train a Markov chain from these data points with the command:

>mc1 = train dna_short :: MarkovChain 2 Char Double

There's a few interesting tidbits here.
Working from right to left, the Double specifies how we will be internally representing our probabilities;
the Char is the type of our data points;
and the number three is called the "order" of our Markov chain.
In a Markov chain, we make the assumption that any given data point is dependent only on some previous number of data points.
In this case, we're only dependent on the 2 previous points.

There are two main ways to use a Markov chain model.
The first is to generate new data.

>fakedna1 = evalRand  (replicateSamples 1000 mc1 "") $ mkStdGen 1

This generates the following strand of dna:

ACGCGCGCGACGCGCGCGCGCTGCGCGCGCGCTATTAGATAGCGATGCGACGCGCGCTATAGCGCGACGCGCGCGACGCGCGCGCTGATATGCTGCTTAT
ATATATGCTAGCTGCTAGCGCGCGCGATGAGCGCGCTAGACGCGCTGCGCGCGCGCGACGCGCGACGCGCGCGCGCGCTAGCGCGCGCTAGCGCGATTTA
GCGCGCTATACGCGCGCGCGCGCGCGCGCTTATACGCTATATAGCGCGATTACGCGCGCGCGCGCGCGCTAGATAGCGATATACGATATGCGCGAGAGCG
CGCGATATTGCGCGCGCTGATACTTAGCGCGCGCGAGATATAGCTGAGCGCGATGATGCGATACGCTATAGCGCGCGATATTGCGCTGCGAGCGCGCGAT
GCGCGCGAGCGATACGCGCGCGCGCGCGCGCGCGCGCGCGCTATGCTACGCGCTGCGCGCGCGCGCGATTACGCGCGCGACGCGACGATAGCGACGATAT
ATTGCGATATATTATACGACGCGCGATTTTGCGCGCGCGCGCGCGCGCGCGCTAGAGCGCGCTAGATAGCGCGCGCGCGCTTATATGCGCTACTTTACGC
GCTAGCGCGCGCGCGAGATATATTTATTAGCGCGCTAGATGCGCGCGACTGCGCGCTGATTTGATAGCGCGCGCTGACGCGCGCGCGCGCGCGCGCGATT
ATAGCGCGCGCGCGATATGCTAGAGATGCTATAGCTATGCGCGCGCGCGCTAGAGATGCGCTATATTAGCGCGCGAGCGCGATATTGACTGCGCGCGACG
CGCGCTATAGCGCGCGCGCGATATGCGCGCGCTAGAGACGATAGCGCGACGCGATGACGCGCGCTTAGCTTGCGCGCGCGCGAGCGCGCGCGCGCGAGCT
TGACTTATATTGATATAGCGCGCTACGCGCGCTAGCTGATACGCGCTATGCGCGCGCTTATGCTTATAGCGCGCGCGCTATTATTGCGCGCGCGCGCTTA

The second way to use the model is to ask it questions.
For example, we might want to ask: given a string of dna, how likely was it to be generated by our model?

Markov chains are different than most other models in the HLearn library because the order of our data points matters.
For example, we could shuffle around the data points inside dna1 to create the data set:

>dna2 = "AAAAAAACCCCCCGGGGGGGTTTTT"
>mc2 = train dna2 :: MarkovChain 3 Char Double

But this is clearly a much different data set.
This property, where order matters, means that Markov chains are non-abelian.

>main = do
>   putStrLn "Initiating IO"
>   dna <- readFile "data/winegrape-chromosome.txt"
>   putStr "Training model... "
>   let model = train (dna) :: MarkovChain 5 Char LogFloat
>   deepseq model $ putStrLn "Done."

>parallel2 :: 
>    ( HomTrainer model
>    , NFData model
>--    , continer ~ []
>--    , CK.Partitionable container
>--    , CK.PartitionableConstraint container datapoint
>    ) => ({-container-} [Datapoint model] -> model) -- ^ sequential batch trainer
>      -> ({-container-} [Datapoint model] -> model) -- ^ parallel batch trainer
>parallel2 train = \datapoint ->
>--     F.foldl' (mappend) mempty $ parMap strat train (CK.partition n datapoint)
>    reduce $ parMap strat train1dp {-(CK.partition n-} datapoint{-)-}
>    where
>        strat = rdeepseq
>        n = unsafePerformIO $ getNumCapabilities
